# üìä An√°lise Completa do Projeto ACE-Step V1.5

## üìã Sum√°rio Executivo

Este documento cont√©m a an√°lise completa do projeto ACE-Step V1.5 para gera√ß√£o de m√∫sica com IA,
incluindo as modifica√ß√µes realizadas para limitar o uso de mem√≥ria RAM a 4GB.

---

## üèóÔ∏è Estrutura do Projeto

```
ACE-Step-1.5/
‚îú‚îÄ‚îÄ acestep/                         # M√≥dulo principal
‚îÇ   ‚îú‚îÄ‚îÄ acestep_v15_pipeline.py     # Pipeline Gradio (UI web)
‚îÇ   ‚îú‚îÄ‚îÄ api_server.py               # Servidor FastAPI (REST API)
‚îÇ   ‚îú‚îÄ‚îÄ handler.py                  # Handler do modelo DiT
‚îÇ   ‚îú‚îÄ‚îÄ llm_inference.py            # Handler do LM (5Hz)
‚îÇ   ‚îú‚îÄ‚îÄ inference.py                # API de infer√™ncia unificada
‚îÇ   ‚îú‚îÄ‚îÄ gpu_config.py               # Configura√ß√£o GPU/mem√≥ria
‚îÇ   ‚îú‚îÄ‚îÄ memory_manager.py           # üÜï Gerenciador de mem√≥ria (4GB)
‚îÇ   ‚îú‚îÄ‚îÄ gradio_ui/                  # Interface Gradio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interfaces/             # Componentes de UI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ events/                 # Handlers de eventos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ i18n/                   # Tradu√ß√µes
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ checkpoints/                    # Modelos baixados
‚îú‚îÄ‚îÄ .env                            # üÜï Configura√ß√£o de mem√≥ria
‚îú‚îÄ‚îÄ .env.example                    # Template original
‚îú‚îÄ‚îÄ start_4gb_mode.sh               # üÜï Script de inicializa√ß√£o
‚îî‚îÄ‚îÄ ANALISE_PROJETO.md              # üÜï Este documento
```

---

## üß† Componentes de Mem√≥ria

### Modelos e Consumo de VRAM

| Modelo | VRAM Necess√°ria | Fun√ß√£o |
|--------|-----------------|--------|
| **DiT (acestep-v15-turbo)** | ~3-4 GB | Modelo de difus√£o para gera√ß√£o |
| **VAE** | ~1-2 GB | Codificador/decodificador de √°udio |
| **LM 0.6B** | ~3 GB | Language Model pequeno |
| **LM 1.7B** | ~8 GB | Language Model m√©dio |
| **LM 4B** | ~12 GB | Language Model grande |
| **Text Encoder** | ~0.5 GB | Codificador de texto |

### Consumo por Opera√ß√£o

| Opera√ß√£o | VRAM Estimada | RAM Estimada |
|----------|---------------|--------------|
| Gera√ß√£o simples (sem LM) | 3-4 GB | 2-3 GB |
| Gera√ß√£o com LM 0.6B | 6-7 GB | 4-5 GB |
| Gera√ß√£o com LM 1.7B | 11-12 GB | 6-8 GB |
| Batch de 2 amostras | 2x o normal | 1.5x o normal |

---

## ‚öôÔ∏è Sistema de Tiers de Mem√≥ria

O projeto usa um sistema de "tiers" baseado na VRAM dispon√≠vel:

### Tier 1 (‚â§4GB) - **CONFIGURADO PARA VOC√ä**
- ‚úÖ Dura√ß√£o m√°xima: 180s (3 minutos)
- ‚úÖ Batch size: 1 (sem batching)
- ‚úÖ LM: Desabilitado
- ‚úÖ Offload para CPU: Habilitado

### Tier 2 (4-6GB)
- Dura√ß√£o m√°xima: 360s (6 minutos)
- Batch size: 1
- LM: Desabilitado

### Tier 3 (6-8GB)
- Dura√ß√£o m√°xima: 240s (4 minutos) com LM / 360s sem LM
- Batch size: 1-2
- LM: 0.6B opcional

### Tier 4+ (>8GB)
- Dura√ß√µes e batch sizes maiores
- LM: 0.6B, 1.7B, 4B dispon√≠veis

---

## üõ†Ô∏è Modifica√ß√µes Realizadas

### 1. Arquivo `.env` (Configura√ß√£o Principal)

```bash
# For√ßar limite de 4GB
MAX_CUDA_VRAM=4

# Desabilitar LM por padr√£o (economiza ~3GB)
ACESTEP_INIT_LM_DEFAULT=false

# Offload para CPU
ACESTEP_OFFLOAD_TO_CPU=true
ACESTEP_OFFLOAD_DIT_TO_CPU=true

# Limites de gera√ß√£o
ACESTEP_MAX_DURATION=180
ACESTEP_MAX_BATCH_SIZE=1

# PyTorch memory management
PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128
```

### 2. M√≥dulo `memory_manager.py` (Novo)

Funcionalidades:
- **Monitoramento de mem√≥ria** em tempo real
- **Valida√ß√£o de par√¢metros** antes da gera√ß√£o
- **Garbage collection for√ßado** ap√≥s cada gera√ß√£o
- **Sincroniza√ß√£o** entre servidor local e web
- **Decorator** para aplicar limites automaticamente

### 3. Script `start_4gb_mode.sh` (Novo)

Menu interativo para:
1. Iniciar Gradio UI (porta 7860)
2. Iniciar API Server (porta 8001)
3. Iniciar ambos os servi√ßos
4. Testar configura√ß√£o de mem√≥ria

---

## üîÑ Sincroniza√ß√£o Local ‚Üî Web

A sincroniza√ß√£o garante que os mesmos limites sejam aplicados:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   SERVIDOR LOCAL    ‚îÇ      ‚îÇ     VERS√ÉO WEB      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ .env carregado      ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ .env carregado      ‚îÇ
‚îÇ memory_manager.py   ‚îÇ      ‚îÇ memory_manager.py   ‚îÇ
‚îÇ gpu_config.py       ‚îÇ      ‚îÇ gpu_config.py       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                            ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   MESMOS LIMITES:       ‚îÇ
         ‚îÇ   - Max 4GB VRAM        ‚îÇ
         ‚îÇ   - Max 180s dura√ß√£o    ‚îÇ
         ‚îÇ   - Batch size: 1       ‚îÇ
         ‚îÇ   - LM: Desabilitado    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Mecanismo de Sincroniza√ß√£o

1. **Vari√°veis de Ambiente**: Ambos os modos leem as mesmas vari√°veis `.env`
2. **GPUConfig Global**: O objeto `GPUConfig` √© compartilhado
3. **MemoryManager Singleton**: Uma √∫nica inst√¢ncia gerencia todos os limites
4. **Valida√ß√£o de Par√¢metros**: Ambos validam com `validate_generation_params()`

---

## üöÄ Como Usar

### Op√ß√£o 1: Script Interativo (Recomendado)

```bash
cd "/home/layton/√Årea de trabalho/ACE 1.5"
./start_4gb_mode.sh
```

### Op√ß√£o 2: Gradio UI Direto

```bash
cd "/home/layton/√Årea de trabalho/ACE 1.5"
source .venv/bin/activate  # Se usar venv

python -m acestep.acestep_v15_pipeline \
    --server-name 0.0.0.0 \
    --port 7860 \
    --init_service true \
    --offload_to_cpu true \
    --language pt
```

### Op√ß√£o 3: API Server

```bash
cd "/home/layton/√Årea de trabalho/ACE 1.5"
source .venv/bin/activate

python -m uvicorn acestep.api_server:app \
    --host 0.0.0.0 \
    --port 8001 \
    --workers 1
```

---

## üìà Otimiza√ß√µes de Mem√≥ria Aplicadas

### 1. Offload para CPU
Quando um modelo n√£o est√° em uso, ele √© movido para RAM:
- DiT ‚Üí CPU ap√≥s gera√ß√£o
- VAE ‚Üí CPU ap√≥s decodifica√ß√£o
- Text Encoder ‚Üí CPU ap√≥s encoding

### 2. Garbage Collection Agressivo
```python
# Ap√≥s cada gera√ß√£o:
gc.collect()
torch.cuda.empty_cache()
torch.cuda.synchronize()
```

### 3. Limites de Aloca√ß√£o PyTorch
```bash
PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128
```

### 4. Desabilita√ß√£o do LM
O Language Model (5Hz LM) consome ~3GB adicionais. Com 4GB, n√£o √© vi√°vel.

### 5. Gera√ß√£o Tiled (Tile-based)
Decodifica√ß√£o VAE em chunks para economizar mem√≥ria:
```python
use_tiled_decode=True  # Padr√£o habilitado
```

---

## ‚ö†Ô∏è Limita√ß√µes do Modo 4GB

| Funcionalidade | Status | Motivo |
|----------------|--------|--------|
| Gera√ß√£o b√°sica de m√∫sica | ‚úÖ Funciona | DiT + VAE cabem |
| Letras customizadas | ‚úÖ Funciona | Texto simples |
| Language Model (thinking=true) | ‚ùå Desabilitado | +3GB necess√°rios |
| Batch >1 amostra | ‚ùå Desabilitado | Mem√≥ria insuficiente |
| Dura√ß√µes >3 min | ‚ùå Limitado | Risco de OOM |
| Audio Cover/Remix | ‚úÖ Funciona | Usa mesma mem√≥ria |
| Multi-modelo | ‚ùå Limitado | Apenas 1 modelo por vez |

---

## üîß Troubleshooting

### Erro: CUDA Out of Memory

```bash
# 1. Verificar uso atual
nvidia-smi

# 2. Matar processos que usam GPU
fuser -k /dev/nvidia*

# 3. Reiniciar o servidor
./start_4gb_mode.sh
```

### Erro: Servidor lento

```bash
# Verificar se offload est√° ativo
grep OFFLOAD .env
# Deve mostrar:
# ACESTEP_OFFLOAD_TO_CPU=true
```

### Mudar para modo mais leve

```bash
# Editar .env
nano .env

# Reduzir dura√ß√£o m√°xima
ACESTEP_MAX_DURATION=120  # 2 minutos
```

---

## üìä Monitoramento

### Via Python

```python
from acestep.memory_manager import get_memory_manager

manager = get_memory_manager()
status = manager.get_status()
print(status)
```

### Via Terminal

```bash
# GPU
watch -n 1 nvidia-smi

# RAM
watch -n 1 "free -h"
```

---

## üìù Pr√≥ximos Passos Sugeridos

1. **Quantiza√ß√£o de Modelos**: Converter para INT8 para reduzir VRAM
2. **Model Sharding**: Dividir modelo entre CPU e GPU
3. **Caching de Resultados**: Evitar recomputa√ß√£o
4. **Compress√£o de √Åudio**: Gerar em qualidade menor primeiro

---

*Documento gerado em: 04/02/2026*
*Configura√ß√£o: ACE-Step V1.5 com limite de 4GB RAM*
